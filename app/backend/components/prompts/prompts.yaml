prompts:
  - name: "[BE][SINGLE] Response time"
    type: default
    place: graph
    prompt: |
      # Role
      Performance response‑time analyst. Evaluate stability of response‑time metrics for tests that should NOT exhibit periodic fluctuations. Allow only a brief warm‑up spike near the start.

      # Input
      An image with a time‑series graph containing lines: `Average response time`, `Median response time`, `90%-tile`.

      # Normalization
      - Focus on the steady‑state window: ignore the early warm‑up phase and final ramp‑down.
      - Warm-up allowance: a single brief spike (or small cluster) near the beginning is expected. Spikes that recur after steady-state begins should be treated as issues.
      - Small random noise within ~±10% during steady-state is normal and should not be labeled as a performance issue.
      - Preserve units as displayed (ms or s) when wording the observation.

      # Tolerances (practical significance)
      Assess only the steady-state window (after warm-up).
      - Average response time (Avg):
        - Stable if values stay within ~±12% of their steady level and show no sustained upward trend.
        - Performance issue if there is a sustained upward trend across ≥5 adjacent points or a prolonged elevation > ~15% across ≥5 adjacent points.
      - Median response time (Median):
        - Stable if it stays within ~±10% with only minimal noise; treat Median as advisory. If Avg and 90%-tile are stable, do not mark Median as a performance issue for minor noise alone.
        - Performance issue only if there is a clear step change or prolonged elevation outside ~±12% across ≥5 adjacent points.
      - 90%-tile response time (P90):
        - Stable if the baseline level is steady within ~±15%; occasional isolated spikes up to about +25% above baseline are acceptable after warm-up.
        - Performance issue if tall spikes persist across ≥5 adjacent points or if the baseline shifts upward > ~20% after warm-up.

      # Decision rule (simple)
      - Decide stability for each metric independently using the tolerances above.
      - Be conservative: ignore the initial warm-up spike and brief transient noise; look for sustained patterns across multiple adjacent points.
      - If a metric is within the tolerances above, you must use the "stable" phrasing and must not describe it as having continuous fluctuations.

      # Output (exact lines)
      Provide one bullet per metric, no intro or conclusion:
      - If no issue: `- The [metric] is consistently stable [without fluctuations/with minimal fluctuations].`
      - If issue: `- The [performance issue description], indicating a performance issue.`
      Use one of the following issue descriptions when applicable: `a sustained upward trend`, `a prolonged elevation beyond tolerance`, `repeated tall spikes during steady-state`, `an upward baseline shift after warm-up`.
      Do not use generic phrases like `continuous fluctuations` when values are within tolerance.

  - name: "[BE][SINGLE] Errors"
    type: default
    place: graph
    prompt: |
      # Role
      Errors throughput analyst. Identify error presence and characterize the pattern (isolated spikes, recurring bursts, or sustained elevation).

      # Inputs
      <image>
      A single time‑series chart of error throughput (e.g., "Num of Errors" or similar). The chart may show totals (e.g., "Total 48k").
      </image>

      # Method (simple and robust)
      1) Read the line and ignore ramp‑up conventions used for performance metrics; for errors, focus on whether the line is at zero vs. above zero and the pattern when above zero.
      2) Pattern taxonomy (choose all that apply; highest priority first):
        - Sustained elevated: the line stays above zero for a prolonged interval (≈ ≥60 seconds) without returning to zero.
        - Recurring bursts: multiple spikes separated by short zero (or near‑zero) gaps, occurring repeatedly across the window; may appear roughly periodic.
        - Isolated spikes: one or few short spikes with long zero intervals between them.
      3) Error floor between events:
        - Returns to zero between spikes → error floor between events returns to zero.
        - Stays elevated between spikes → error floor between events stays elevated.
      4) Temporal distribution (qualitative):
        - Single interval (front‑loaded/early, middle, or late).
        - Scattered across the full window.
        - Rough periodicity (if spacing between bursts looks uniform).
      5) Optional numeric echo:
        - If the chart prints totals/means/labels (e.g., "Total 48k"), you MAY echo them exactly as displayed. Do not estimate or infer numbers.

      # Decision rules
      - Errors present if the line rises above zero at any point; otherwise, No errors.
      - If multiple patterns apply, report the highest‑priority one first: Sustained elevated > Recurring bursts > Isolated spikes.
      - Prefer concise qualitative descriptions over invented quantities.

      # Output (exact; produce 2 or 3 bullets, no intro or conclusion)
      Always output the first two bullets. Output the third only if a total/printed number is visible on the chart.

      - Status: [No errors detected.] | [Errors present — pattern: {Sustained elevated | Recurring bursts | Isolated spikes}; error floor between events [returns to zero | stays elevated].]
      - Temporal distribution: [single interval (early/middle/late) | scattered across the window | recurring bursts roughly periodically]. If timestamps are legible, include 1–2 representative times (e.g., "around 08:05"), otherwise omit times.
      - Totals: [echo the chart’s printed total/mean text exactly, e.g., "Total 48k"]. (Include this bullet only if such text is visible.)

      # Style and constraints
      - Concise, formal; one sentence per bullet.
      - Do not invent numeric values; only echo numbers exactly as printed on the chart.
      - Treat brief single‑point noise as non‑material unless part of a clear pattern.
      - Do not reference other metrics or charts.

      # Final validation checklist (apply before responding)
      - Bullet count is 2 if no printed totals are visible; otherwise 3.
      - If "No errors detected" is reported, there must be no visible excursions above zero.
      - If "Recurring bursts" or "Isolated spikes" are reported, the between‑event error floor is noted (returns to zero vs stays elevated).
      - No extra text before or after the bullets.

  - name: "[BE][SINGLE] Throughput"
    type: default
    place: graph
    prompt: |
      # Role
      Performance throughput analyst. Evaluate stability of throughput metrics for tests that should NOT exhibit periodic Throughput oscillations.

      # Input
      An image with a time-series graph containing lines: `Throughput`, `Users`.

      # Normalization
      - Identify the steady-state window as the period where `Users` is flat (plateau). Ignore ramp-up and ramp-down.
      - This test class expects Throughput to be effectively flat in steady-state. Allow only small random noise; no periodic oscillation is expected.

      # Tolerances (practical significance)
      Assess only the steady-state window.
      - Throughput:
        - Stable if values stay within ~±10% of their steady level and show no sustained downward trend.
        - Performance issue if there is a sustained downward trend or a prolonged drop beyond ~15% across multiple adjacent points.
      - Users:
        - Stable if flat at the target level during steady-state.
        - Performance issue if it drops, oscillates materially, or fails to reach/maintain the plateau.

      # Decision rule (simple)
      - Decide stability for each metric independently using the tolerances above.
      - Be conservative: do not flag transient spikes or brief dips; look for sustained patterns across multiple adjacent points.

      # Output (exact lines)
      Provide one bullet per metric, no intro or conclusion:
      - If no issue: `- The [metric] is consistently stable [without fluctuations/with minimal fluctuations].`
      - If issue: `- The [performance issue description], indicating a performance issue.`

  - name: "[BE][COMPARISON] Template summary"
    type: default
    place: template
    prompt: |
      # Role
      You are the final performance test summarizer. Combine prior observations into a single, executive-grade report with a gating decision.

      # Inputs
      You will receive one or more observation blocks collected earlier. Each block is optional:
      - <aggregated_data>…</aggregated_data> — either a single line stating no degradation or a list of degraded transactions.
      - <graphs>…</graphs> — bulleted observations from summary statistics, throughput, response time, CPU, MEM, application panels, etc.
      - <ml_analysis>…</ml_analysis> — a structured list of ML checks describing throughput/response-time trends and possible anomalies; each item typically has fields like status/method/description/value. Treat status = "failed" as an ML event; only events that indicate increases/spikes in response time are counted as critical. Events that indicate decreases/drops are not critical.

      Use only the information contained in these blocks. Do not recalculate metrics or introduce external data.

      # Gating logic
      Derive four decisions in this order:
      1) Comparable vs Not comparable (server-side):
        - If <graphs> explicitly says “comparable” → comparable.
        - If it says “not comparable” → not comparable.
        - Otherwise (missing/ambiguous): comparable.
        - Comparability is derived solely from <graphs>; do not override this based on <aggregated_data> or <ml_analysis>.
      2) Issue tally (counts used for status decision):
        - Degraded transactions (from <aggregated_data>): count distinct transactions with degradation (deduplicate multiple metrics for the same transaction). If a line matches
          "- The (avg|pct50|pct75|pct90) for (.+?) has increased" → extract the transaction name from capture group 2 and count unique names only. If no clear pattern is present, fall back to counting bullet lines starting with “- ” as a proxy. Do not upweight severity based on the number of metrics for the same transaction.
        - ML critical anomalies (from <ml_analysis>): count items with status = "failed" that indicate response-time increases or spikes. Use these verb cues as critical: ["increase","increased","increasing","rise","rising","spike","spiked","surge","surged","higher than","rose","peaked"]. Explicitly EXCLUDE decreases/drops: ["decrease","decreased","decreasing","drop","dropped","dropping","lower","lowered","fell","fall","fallen","decline","declined","declining"].
        - Ignore adjectives such as "significant", "severe", or "major" in <aggregated_data> and <ml_analysis> when computing status; they are descriptive only.
      3) Test status (Acceptable vs Unacceptable):
        - Acceptable if results are comparable AND degraded_transactions ≤ 2 AND ml_critical_anomalies ≤ 2. Do NOT override this with narrative severity; this rule is binding. Magnitude of degradation (e.g., +113.6%) does not change this decision when counts are within thresholds.
        - Unacceptable if results are not comparable.
        - Unacceptable if degraded_transactions ≥ 3 OR ml_critical_anomalies ≥ 3.
        - If one or both input blocks (<aggregated_data>, <ml_analysis>) are missing, decide using the available counts together with comparability.
        - Binding pseudocode (authoritative):
          - let status = "Unacceptable"
          - if comparable == true and degraded_transactions ≤ 2 and ml_critical_anomalies ≤ 2 → set status = "Acceptable"
          - emit status exactly as computed above.
      4)

      # Output format (exact; DO NOT include any <report> tags; the final output must start with "<h2>Executive summary </h2>")
      <h2>Executive summary </h2>
      - Test status: [Acceptable/Unacceptable]
      - Server-side test results were [comparable/not comparable] to the baseline test. The 90th-percentile response time was [current p90 response time], previously [baseline p90 response time].

      <h3>Detailed Observations</h3>
      - Performance Metrics:
        - [One sentence summarizing throughput/response time behavior from <graphs>. End with a period.]
        - [If <ml_analysis> is present: One sentence beginning with "ML analysis:" summarizing trend findings (e.g., stable/constant). If any items have status = "failed", include up to two anomalies as short clauses that state exactly what happened using a past‑tense action verb (normalize "increasing/decreasing" to "increased/decreased"), for example: "{metric} increased to {value} during {window}" or "{metric} decreased to {value} during {window}". If the description includes a start and end timestamp, format the window as "from {start} to {end}" or "at {time}". If no explicit verb is present, use "changed to". End with a period.]

      <h3>Top degraded transactions</h3>
      (INCLUDE THIS ONLY IF PROVIDED IN <aggregated_data> AND THERE IS AT LEAST ONE DEGRADED TRANSACTION; DO NOT WRITE "None.")
      - {transaction}: {metric} increased to {value} ms (from {baseline} ms) by {diff_pct}%.

      - If a transaction has multiple degraded metrics (e.g., avg/pct50/pct75/pct90), output a single line per transaction and choose pct50 (median) if present; otherwise pick a single representative metric.

      <h3>Recommendations</h3>
      (INCLUDE THIS ONLY IF TEST IS NOT COMPARABLE AND STATUS IS UNACCEPTABLE)
      - [Actionable, data-backed steps (3–5 bullets). Reference the observed issues directly; avoid generic advice.]

      # Style and constraints
      - Be concise and formal; use only the exact headings and bullet structure shown above.
      - Allowed headings are exactly: "<h2>Executive summary </h2>", "<h3>Detailed Observations</h3>", "<h3>Top degraded transactions</h3>", and "<h3>Recommendations</h3>" (the last two are conditional as stated). Do not add any other sections (e.g., "Gating logic").
      - Use only data explicitly present in the inputs; do not invent numbers.
      - If any section has no relevant content, omit its bullets entirely (leave the heading as-is).
      - If 90th-percentile response times are present in <graphs>, echo them exactly as written (e.g., “2,690 ms”).
        - When <ml_analysis> is provided, include exactly one Performance Metrics bullet that begins with "ML analysis:" and follows the anomaly rule (max two anomalies, single sentence).
        - If Test status is Acceptable, avoid subjective severity terms (e.g., "critical", "cannot be ignored", "instability"). Describe degraded transactions and anomalies factually.

      # Final validation checklist (apply before responding)
      - Do NOT include any of these tags or placeholders in the final output: <report>, <graphs>, <aggregated_data>, {aggregated_data_analysis}, {graphs_analysis}.
      - Ensure the first line of the output is exactly the heading "<h2>Executive summary </h2>" (no preceding tags).
      - Under each included subsection, ensure every bullet starts with "- " and is a complete sentence ending with a period.
      - Ensure the first bullet under the Executive summary section is the Test status line.
      - Ensure the second bullet under the Executive summary section states the comparability line exactly as specified.
      - If <aggregated_data> indicates "no degraded transactions" (e.g., contains the phrase "For all transactions, the response time remained at the same level."), remove the entire "Top degraded transactions" section from the output. Do not write "None."
      - If <ml_analysis> is present, verify there is a Performance Metrics bullet that begins with "ML analysis:" and, if applicable, lists up to two anomalies in a single sentence.
      - If anomalies are listed, ensure each includes a past‑tense action verb (e.g., increased/decreased/spiked), the value, and a time window formatted as "from {start} to {end}" or "at {time}".
      - Enforce status: if Comparable = comparable AND degraded_transactions ≤ 2 AND ml_critical_anomalies ≤ 2, the Test status line MUST be "Acceptable" and no other text may label the test "Unacceptable" or imply critical severity.
      - The narrative must align with the computed status. Do not write "Unacceptable" unless one of the explicit conditions in Test status is met.
      - Do not state that results are "not comparable" unless <graphs> explicitly says so.
      - Do not escalate severity because multiple metrics within the same transaction degraded; count by unique transaction only.
      - Omit the "Recommendations" section entirely unless the Test status is Unacceptable.
      - If Test status is Acceptable, the output MUST NOT contain a "<h3>Recommendations</h3>" section.
      - Do not include any additional narrative sections (e.g., "Gating logic"). Only the allowed headings may appear in the output.
      - Final auto-correction: If the computed status is Acceptable but the drafted output contains "Unacceptable" or a "<h3>Recommendations</h3>" section, replace the status with "Acceptable" and remove the "<h3>Recommendations</h3>" section, then regenerate the output.
        - For "Top degraded transactions": ensure each transaction appears at most once; when multiple metrics are degraded for the same transaction, prefer pct50 (median) if available; otherwise select a single representative metric.

      Observations from aggregated data analysis:
      <aggregated_data>
      ${aggregated_data_analysis}
      </aggregated_data>

      Observations from graphs analysis:
      <graphs>
      ${graphs_analysis}
      </graphs>

      Observations from ML analysis:
      <ml_analysis>
      ${ml_summary}
      </ml_analysis>

  - name: "[BE][COMPARISON] Aggregated data"
    type: default
    place: aggregated_data
    prompt: |
      # Role
      Combined performance comparator and degradation summarizer. From a single JSON array, decide comparability (Current vs Baseline) using overview metrics and list degraded transactions from aggregated metrics.

      # Inputs
      <json>
      A JSON array where each element has the shape:
      {
        "transaction": "Average" | "Median" | "75%-tile" | "90%-tile" | "RPS" | "Total requests" | "Error rate" | <transaction_name>,
        "metrics": {
          "overview_data": { "Value": {"value": <num>, "baseline": <num>, "diff_pct": <num?>} }  // for high-level rows
          OR
          "aggregated_data": {
            "avg"|"pct50"|"pct75"|"pct90"|"errors"|"rpm"|"count"|"stddev": {"value": <num>, "baseline": <num>, "diff_pct": <num?>}
          }
        }
      }
      At least one overview metric must be present. Aggregated transaction entries are optional.
      </json>

      # Definitions
      - diff_pct_field = the numeric diff_pct provided in the JSON (unrounded), if present.
      - diff_pct_calc = 100 * (value − baseline) / baseline.
      - diff_pct_used = diff_pct_field if present and numeric; else diff_pct_calc.
      - For RPS, a NEGATIVE diff implies a drop (worse). For latency metrics, a POSITIVE diff implies slower (worse).

      # Parsing
      - Build a dictionary of overview metrics by name: {Average, Median, 75%-tile, 90%-tile, RPS, Total requests, Error rate} when present.
      - Build a list of aggregated transactions: each with any of {avg, pct50, pct75, pct90, errors, rpm, count, stddev} present.

      # Tolerances (practical significance)
      Compute percent change as diff_pct_used.
      - Average, 75%-tile, 90%-tile response time: |Δ| ≤ 12% → within tolerance; > 12% → significant.
      - Median response time: advisory only. If it is the only metric out of tolerance while avg and p75 are within tolerance, treat overall as comparable.
      - RPS (Requests per second): |Δ| ≤ 5% → within tolerance; DROP > 5% (Δ < −5%) → significant.
      - Error rate (0–100 scale):
        - If Baseline < 0.5%: ignore changes < 0.20 percentage points (pp).
        - Otherwise significant if absolute change > 0.50 pp or relative change > 50%.
      - Total requests: informational only; mention if |Δ| > 5% but never fail on this alone.

      # Decision rule (overview comparability)
      - Consider the following as decision metrics: {Average, 75%-tile, 90%-tile, Error rate, RPS}. Median is advisory only.
      - Status = not comparable if:
        - Two or more decision metrics are significant; OR
        - A single severe breach occurs: any of {Average, 75%-tile, 90%-tile} > +20%, or Error rate +1.0 pp or more, or RPS drop > 10% (Δ < −10%).
      - Otherwise, status = comparable.

      # Transaction degradation policy (aggregated metrics)
      - Latency metrics {avg, pct50, pct75, pct90}: a transaction is degraded if ANY:
        - Two or more latency metrics are Significant (diff_pct_used ≥ +12.0), OR
        - Any latency metric is Severe (diff_pct_used ≥ +20.0).
      - Errors (0–100 scale): list only if value > baseline AND (absolute increase ≥ 0.5 pp OR relative increase ≥ 50%).
      - Do NOT list rpm or count.
      - stddev alone is never listed. You MAY append "(higher variability)" to a listed latency metric only if stddev increases ≥ 100% AND that metric increases ≥ 5%.

      # Output (exact; no prologue/epilogue)
      1) Server-side test results are <comparable|not comparable> to the baseline test. The <headline_metric> is now <current>, previously <baseline>.
        - headline_metric selection: use 90th-percentile response time if present; else Average response time; else 75%-tile response time.
      2) Only if not comparable: Significant changes: <Metric A>: <value change> (<percent change>), <Metric B>: <value change> (<percent change>).
        - Choose up to two metrics among {Average, 75%-tile, 90%-tile, Error rate, RPS} that triggered the decision, ordered by severity.
        - value change format: "from <baseline> to <value>"; percent change: use rounded diff_pct_used with one decimal and a sign (e.g., +13.2%).
      3) If there are degraded transactions (per policy), output exactly:
      Transactions with declining performance:
      - The {metric} for {transaction} has increased by {diff_pct}% (from {baseline} to {value}).
      - [Repeat one bullet per degraded metric per transaction. List only metrics that triggered the rule.]
      4) If there are NO degraded transactions, output exactly:
      For all transactions, the response time remained at the same level.

      # Style
      - Use the verb "increased" for degraded latency metrics.
      - Never use: declined, dropped, decrease, reduced.
      - Round displayed {diff_pct}% to one decimal; do not round for threshold checks.
      - Echo baseline/value exactly as given in JSON.

      # Validation checklist (must pass)
      - diff_pct_used is computed as specified where missing.
      - Comparability status follows the decision rule exactly.
      - Line 1 uses the correct headline_metric per availability.
      - Line 2 is present only when status is "not comparable" and lists up to two triggering metrics.
      - Transaction section follows the degradation policy and wording precisely.
      - Output contains only the lines described above (no extra commentary).

      # Aggregated data:

  - name: System message
    type: default
    place: system
    prompt: |
      Act as a skilled Performance Analyst with advanced data analysis expertise to interpret performance test results.

  - name: "[FE][COMPARISON] Aggregated data"
    type: default
    place: aggregated_data
    prompt: |
      # Role
      Combined front-end performance summarizer. From a single JSON array, (1) summarize top-line FE metric changes (overview_data) and (2) list degraded transactions with likely contributors (aggregated per-transaction metrics).

      # Input
      <json>
      A JSON array; each element has one of these shapes:
      {
        "transaction": "FCP" | "LCP" | "TTFB" | "Fully Loaded" | "Total Requests" | "Third-Party Requests" | "Total Transfer Size (KB)" | "Transfer Size for CSS (KB)" | "Transfer Size for Image (KB)" | ...,
        "metrics": {
          "overview_data": { "Value": {"value": <num>, "baseline": <num>, "diff_pct": <num?>} }
        }
      }
      {
        "transaction": "<page or route>",
        "metrics": {
          "google_web_vitals": { "LCP": {..}, "TTFB": {..}, "FCP": {..}, ... },
          "first_party_transfer_size": { "css": {...}, "javascript": {...}, "image": {...}, "html": {...}, "font": {...}, "plain": {...} },
          "third_party_transfer_size": { same keys as above }
        }
      }
      </json>

      # Definitions
      - diff_pct_field = numeric diff_pct from JSON if present; else null.
      - diff_pct_calc = 100 * (value − baseline) / baseline when both numbers exist.
      - diff_pct_used = diff_pct_field if numeric else diff_pct_calc.
      - Positive latency diffs (LCP/TTFB/FCP) = slower; negative = faster.

      # Overview evaluation (top-line)
      - Focus on LCP and TTFB (primary FE UX signals). FCP and Fully Loaded are informational only.
      - Significant degradation threshold (either condition):
        - LCP: diff_pct_used ≥ +12.0 OR absolute increase ≥ 200 ms.
        - TTFB: diff_pct_used ≥ +12.0 OR absolute increase ≥ 150 ms.
      - Transfer-size totals (informational flag only):
        - Flag if "Total Transfer Size (KB)" increases by ≥ +10% and ≥ +50 KB.

      Status rule (overview comparability)
      - Not comparable if LCP or TTFB meets the significant threshold above (either condition).
      - Comparable otherwise.

      # Per-transaction evaluation (reused FE policy)
      - Use `google_web_vitals` for LCP and TTFB and (for LCP only) correlate with transfer sizes.
      - LCP significance: diff_pct_used ≥ +12.0 OR absolute increase ≥ 200 ms; Severe if ≥ +20.0 OR ≥ +400 ms.
      - TTFB significance: diff_pct_used ≥ +12.0 OR absolute increase ≥ 150 ms.
      - Degradation is reported only for increases (positive diffs). Improvements are not listed.

      Correlation for LCP (transfer sizes)
      - When a transaction’s LCP is significant or severe, consider size increases across first/third party:
        - javascript: abs ≥ 200 KB AND rel ≥ 15%
        - css: abs ≥ 100 KB AND rel ≥ 15%
        - image: abs ≥ 100 KB AND rel ≥ 15%
        - html/json ("html" or "plain"): abs ≥ 50 KB AND rel ≥ 20%
      - Mention up to two strongest candidates by absolute KB increase as “Possible contributors”. If none meet thresholds, omit the clause.

      # Output (exact; no intro or ending text; DO NOT use square brackets in the final output)
      1) Front-end performance results are <comparable|not comparable> to the baseline test. The LCP is now <current LCP>, previously <baseline LCP>.
        - Determine <comparable|not comparable> using the overview comparability status rule above.
        - Obtain <current LCP> and <baseline LCP> from the overview LCP entry; echo as integers in ms with the suffix "ms" (e.g., 549 ms). If the LCP overview entry is missing, use "unknown" for the missing value(s).
      2) If there are no degraded transactions (by per-transaction rules): Performance is good across all transactions.
        Otherwise output exactly:
      Transactions with degraded FE performance:
      - {transaction}: <clauses>.
        - Build <clauses> as follows (no brackets in output):
          - Always include the triggering metric(s) in order: LCP increased by {diff_pct_LCP}% (from {baseline_LCP} ms to {value_LCP} ms); TTFB increased by {diff_pct_TTFB}% (from {baseline_TTFB} ms to {value_TTFB} ms).
          - If only one metric triggers, output only that single clause (no trailing semicolon).
          - If contributors are present for LCP, append: " Possible contributors: {resource A} +{abs_increase_A} KB (+{rel_A}%); {resource B} +{abs_increase_B} KB (+{rel_B}%)." (omit if none).

      # Style
      - Concise and formal. One bullet per degraded transaction.
      - Round percentages to one decimal place. Echo ms values as integers.
      - For KB, convert bytes to KB = value/1024 and round to nearest integer. Echo overview KB as provided in the table.
      - Resource names should be prefixed with “first-party” or “third-party” when applicable.
      - Do not include square brackets [] in the final output.

      # Validation checklist
      - Line 1 status strictly follows the overview comparability rule (LCP/TTFB thresholds).
      - Line 1 reports LCP values from the overview entry as "<value> ms" or "unknown" if missing; no brackets are present.
      - Per-transaction bullets include only increased metrics (positive diffs) and conform to thresholds.
      - At most two contributor resources per LCP bullet and only when their thresholds are met.
      - No additional commentary beyond the lines described.
      - The final output contains no square brackets [].

      # Aggregated data:

  - name: "[FE][COMPARISON] Template summary"
    type: default
    place: template
    prompt: |
      # Role
      You are the final front‑end performance summarizer. Consolidate earlier FE observations into a concise report for stakeholders.

      # Inputs
      Provide the single block below (optional but preferred):
      - <aggregated_data>…</aggregated_data> — front‑end comparison and any degraded transaction bullets.
        - Expected contents:
          - Line 1: "Front-end performance results are <comparable|not comparable> to the baseline test. The LCP is now <X>, previously <Y>."
          - Optional section: "Transactions with degraded FE performance:" followed by one bullet per degraded transaction.
      Use only the information contained in this block. Do not invent numbers.

      # Gating logic
      1) Parse comparability and LCP from <aggregated_data> line 1:
        - Pattern: "Front-end performance results are {status} to the baseline test. The LCP is now {X}, previously {Y}."
        - Set overall status = {status} (either "comparable" or "not comparable").
        - Set current_lcp = X and baseline_lcp = Y exactly as written.
        - If <aggregated_data> is missing or the pattern cannot be found, set status = "comparable" and both LCP values to "unknown".
      2) Degraded transactions (optional):
        - If the block contains the section header "Transactions with degraded FE performance:", capture up to two subsequent bullets and include them under Detailed observations.

      # Output format (exact; DO NOT include any <…> tags; the final output must start with "Summary:")
        <h2>Summary:</h2>
        - Front-end test results are {comparable|not comparable} to the baseline test.
        - The LCP is now {current LCP}, previously {baseline LCP}.
        - Detailed observations:
          - Provide one concise sentence summarizing the most material FE finding (e.g., mention if any overview metric increased and whether transfer size flags were present in the first line if stated).
          - If degraded transactions are present in <aggregated_data>, include up to two bullets copied or paraphrased concisely (one line each). Each bullet must be a complete sentence ending with a period.

      # Style and constraints
      - Be concise and formal; use the exact headings and bullet structure shown above.
      - Do not include any <aggregated_data> tags in the final output.
      - Echo LCP values exactly as written in the first line within <aggregated_data> (e.g., “549 ms” or “unknown”).
      - Do not list more than two degraded transactions; prefer the largest LCP regressions or strongest wording present.
      - Do not include relative percentages for CPU/MEM in this FE summary.
      - Degraded transactions included under "Detailed observations" are informational only and do not change the overall comparability status.

      # Final validation checklist (apply before responding)
      - Ensure the first non-empty line is exactly "<h2>Summary:</h2>".
      - Ensure every bullet starts with "  - " (two spaces, a hyphen, and a space) and ends with a period.
      - Ensure no <aggregated_data> tags appear in the final output.
      - Ensure no square brackets are present in the final output.
      - Ensure status and LCP values are derived from <aggregated_data>, or "unknown" when unavailable.

      # Input block
      Front-end report (optional):
      <aggregated_data>
      ${aggregated_data_analysis}
      </aggregated_data>
